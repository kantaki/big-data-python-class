{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Analysis\n",
    "Crawler created using the webcrawler provided and discussed in the following jupyter notebook ( https://github.com/pschragger/big-data-python-class/blob/master/Lectures/Lecture_7_-_Link_Analysis.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Scrappy Project\n",
    "Run the following command to set up a new Scrapy project: csVillanova (Following the tutorial at https://doc.scrapy.org/en/latest/intro/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "scrapy startproject medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spider\n",
    "Go to the newly created folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cd medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute command for generating first spider with <B>name</B> and the <B>domain of the site</B> to be crawled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "scrapy genspider news medium.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading spider file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load medium/medium/spiders/news.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = 'news'\n",
    "    allowed_domains = ['www.olx.com.pk']\n",
    "    start_urls = ['https://www.olx.com.pk/computers-accessories/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made some changed to the computer.py file so that the spider successfully crawls to next pages\n",
    "+ following the tutorial at https://medium.com/python-pandemonium/develop-your-first-web-crawler-in-python-scrapy-6b2ee4baf954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load medium/medium/spiders/news.py\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "class NewsSpider(CrawlSpider):\n",
    "    name = 'news'\n",
    "    allowed_domains = ['www.olx.com']\n",
    "    start_urls = ['https://www.olx.com/computers-accessories/','https://www.olx.com/games-entertainment/']\n",
    "\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=(), restrict_css=('.pageNextPrev',)),\n",
    "             callback=\"parse_item\",\n",
    "             follow=True),)\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        print('Processing..' + response.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on Stackoverflow website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "#running scrappy to get a response from a URL\n",
    "r = requests.get('http://stackoverflow.com')\n",
    "response = TextResponse(r.url, body=r.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://stackoverflow.com/>\n"
     ]
    }
   ],
   "source": [
    "print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//title' data=u'<title>Stack Overflow - Where Developers'>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using xpath to access data\n",
    "response.xpath('//title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Stack Overflow - Where Developers Learn, Share, & Build Careers']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " response.xpath('//title/text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//ul/li' data=u'<li>\\r\\n                            <div c'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"related-site\">\\r\\n             '>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\n                        <a href=\"/t'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\n                    <a href=\"/help\"'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\n                            <a href'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\n                            <a href'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\n                            <a href'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li>\\r\\n                <div class=\"favico'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"dno js-hidden\">\\r\\n            '>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"/questions\" c'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://stack'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://stack'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://stack'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"/help\" class='>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a onclick=\\'StackExcha'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://www.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://www.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://www.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://www.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"#\" class=\"-li'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"#\" class=\"-li'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"#\" class=\"-li'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"#\" class=\"-li'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"#\" class=\"-li'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://stack'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://serve'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://super'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://webap'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://askub'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://webma'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://gamed'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://tex.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://softw'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://unix.'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://apple'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://wordp'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://gis.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://elect'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://andro'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://secur'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://dba.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://drupa'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://share'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://ux.st'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://mathe'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://sales'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://expre'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://pt.st'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://blend'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://netwo'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://crypt'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://coder'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://magen'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://softw'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://dsp.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://emacs'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://raspb'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://ru.st'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://codeg'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://es.st'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://ether'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://datas'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://ardui'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://bitco'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\">\\n                    <'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://photo'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://scifi'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://graph'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://movie'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://music'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://world'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://cooki'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://diy.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://money'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://acade'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://law.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\">\\n                    <'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://engli'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://skept'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://judai'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://trave'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://chris'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://ell.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://japan'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://gamin'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://bicyc'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://rpg.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://anime'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://puzzl'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://mecha'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\">\\n                    <'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://matho'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://math.'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://stats'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://csthe'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://physi'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://chemi'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://biolo'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://cs.st'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://philo'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\">\\n                    <'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://meta.'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://stack'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://api.s'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://data.'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://area5'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a class=\"js-gps-track'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://www.f'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://twitt'>,\n",
       " <Selector xpath='//ul/li' data=u'<li class=\"-item\"><a href=\"https://linke'>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//ul/li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'https://stackoverflow.com',\n",
       " u'https://meta.stackoverflow.com',\n",
       " u'/tour',\n",
       " u'/help',\n",
       " u'https://meta.stackoverflow.com',\n",
       " u'https://stackoverflow.com/company/about',\n",
       " u'https://www.stackoverflowbusiness.com/?ref=topbar_help',\n",
       " u'https://travel.stackexchange.com/questions/105799/can-i-travel-back-to-my-homeland-after-being-granted-germany-refugee-status',\n",
       " u'https://opensource.stackexchange.com/questions/6271/calling-gpld-programs-from-a-shell-script',\n",
       " u'https://aviation.stackexchange.com/questions/46092/why-dont-aircraft-use-nuclear-propulsion',\n",
       " u'https://academia.stackexchange.com/questions/99417/how-to-prevent-students-from-using-modified-calculators-to-cheat-on-exams',\n",
       " u'https://codegolf.stackexchange.com/questions/149137/minimal-sparse-rulers',\n",
       " u'https://worldbuilding.stackexchange.com/questions/98363/if-the-romans-found-one-working-steam-engine-would-they-have-been-able-to-copy-a',\n",
       " u'https://devops.stackexchange.com/questions/2693/set-minikube-to-auto-start-on-windows-log-on',\n",
       " u'https://politics.stackexchange.com/questions/26289/has-there-ever-been-a-documented-instance-of-the-problem-that-net-neutrality-pur',\n",
       " u'https://chemistry.stackexchange.com/questions/86413/enthalpy-calculation',\n",
       " u'https://math.stackexchange.com/questions/2538284/projective-modules-and-ring-homomorphisms',\n",
       " u'https://math.stackexchange.com/questions/2537878/converging-trigonometric-result',\n",
       " u'https://cs.stackexchange.com/questions/84487/what-happens-when-an-internet-connection-is-faster-than-the-storage-write-speed',\n",
       " u'https://math.stackexchange.com/questions/2538337/proving-distance-between-any-two-vertices-is-at-most-2',\n",
       " u'https://electronics.stackexchange.com/questions/341843/adding-a-resistor-to-reduce-crossover-distortion-in-an-lm324-lm358',\n",
       " u'https://math.stackexchange.com/questions/2538184/proof-of-golden-rectangle-inside-an-icosahedron',\n",
       " u'https://english.stackexchange.com/questions/419813/do-i-have-to-put-a-comma-before-in-which-here',\n",
       " u'https://english.stackexchange.com/questions/419897/struggling-in-a-cae-question-on-a-really-confusing-key-answer',\n",
       " u'https://interpersonal.stackexchange.com/questions/7089/how-can-i-politely-tell-a-family-who-invited-me-for-dinner-that-im-still-hungry',\n",
       " u'https://cooking.stackexchange.com/questions/85802/how-to-prevent-liquids-from-spilling-when-pouring-from-measuring-cup',\n",
       " u'https://academia.stackexchange.com/questions/99258/is-it-wrong-to-start-your-abstract-with-a-question',\n",
       " u'https://tex.stackexchange.com/questions/403197/remove-the-chapter-number-in-toc',\n",
       " u'https://reverseengineering.stackexchange.com/questions/16844/how-to-get-a-nice-stack-view-in-radare2',\n",
       " u'https://scifi.stackexchange.com/questions/174987/book-about-a-professor-who-summons-a-female-demon',\n",
       " u'https://unix.stackexchange.com/questions/405783/why-does-man-print-gimme-gimme-gimme-at-0030',\n",
       " u'/questions',\n",
       " u'https://stackoverflow.com/jobs',\n",
       " u'https://stackoverflow.com/jobs/directory/developer-jobs',\n",
       " u'https://stackoverflow.com/jobs/salary',\n",
       " u'/help',\n",
       " u'https://www.stackoverflowbusiness.com/talent?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation&utm_term=sotob2b-footer-talent',\n",
       " u'https://www.stackoverflowbusiness.com/advertise?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation&utm_term=sotob2b-footer-advertise',\n",
       " u'https://www.stackoverflowbusiness.com/enterprise?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation&utm_term=sotob2b-footer-enterprise',\n",
       " u'https://www.stackoverflowbusiness.com/insights?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation&utm_term=sotob2b-footer-insights',\n",
       " u'https://stackoverflow.com/company/about',\n",
       " u'https://stackoverflow.com/company/press',\n",
       " u'https://stackoverflow.com/company/work-here',\n",
       " u'https://stackexchange.com/legal',\n",
       " u'https://stackexchange.com/legal/privacy-policy',\n",
       " u'https://stackoverflow.com/company/contact',\n",
       " u'#',\n",
       " u'#',\n",
       " u'#',\n",
       " u'#',\n",
       " u'#',\n",
       " u'https://stackoverflow.com',\n",
       " u'https://serverfault.com',\n",
       " u'https://superuser.com',\n",
       " u'https://webapps.stackexchange.com',\n",
       " u'https://askubuntu.com',\n",
       " u'https://webmasters.stackexchange.com',\n",
       " u'https://gamedev.stackexchange.com',\n",
       " u'https://tex.stackexchange.com',\n",
       " u'https://softwareengineering.stackexchange.com',\n",
       " u'https://unix.stackexchange.com',\n",
       " u'https://apple.stackexchange.com',\n",
       " u'https://wordpress.stackexchange.com',\n",
       " u'https://gis.stackexchange.com',\n",
       " u'https://electronics.stackexchange.com',\n",
       " u'https://android.stackexchange.com',\n",
       " u'https://security.stackexchange.com',\n",
       " u'https://dba.stackexchange.com',\n",
       " u'https://drupal.stackexchange.com',\n",
       " u'https://sharepoint.stackexchange.com',\n",
       " u'https://ux.stackexchange.com',\n",
       " u'https://mathematica.stackexchange.com',\n",
       " u'https://salesforce.stackexchange.com',\n",
       " u'https://expressionengine.stackexchange.com',\n",
       " u'https://pt.stackoverflow.com',\n",
       " u'https://blender.stackexchange.com',\n",
       " u'https://networkengineering.stackexchange.com',\n",
       " u'https://crypto.stackexchange.com',\n",
       " u'https://codereview.stackexchange.com',\n",
       " u'https://magento.stackexchange.com',\n",
       " u'https://softwarerecs.stackexchange.com',\n",
       " u'https://dsp.stackexchange.com',\n",
       " u'https://emacs.stackexchange.com',\n",
       " u'https://raspberrypi.stackexchange.com',\n",
       " u'https://ru.stackoverflow.com',\n",
       " u'https://codegolf.stackexchange.com',\n",
       " u'https://es.stackoverflow.com',\n",
       " u'https://ethereum.stackexchange.com',\n",
       " u'https://datascience.stackexchange.com',\n",
       " u'https://arduino.stackexchange.com',\n",
       " u'https://bitcoin.stackexchange.com',\n",
       " u'https://stackexchange.com/sites#technology',\n",
       " u'https://photo.stackexchange.com',\n",
       " u'https://scifi.stackexchange.com',\n",
       " u'https://graphicdesign.stackexchange.com',\n",
       " u'https://movies.stackexchange.com',\n",
       " u'https://music.stackexchange.com',\n",
       " u'https://worldbuilding.stackexchange.com',\n",
       " u'https://cooking.stackexchange.com',\n",
       " u'https://diy.stackexchange.com',\n",
       " u'https://money.stackexchange.com',\n",
       " u'https://academia.stackexchange.com',\n",
       " u'https://law.stackexchange.com',\n",
       " u'https://stackexchange.com/sites#lifearts',\n",
       " u'https://english.stackexchange.com',\n",
       " u'https://skeptics.stackexchange.com',\n",
       " u'https://judaism.stackexchange.com',\n",
       " u'https://travel.stackexchange.com',\n",
       " u'https://christianity.stackexchange.com',\n",
       " u'https://ell.stackexchange.com',\n",
       " u'https://japanese.stackexchange.com',\n",
       " u'https://gaming.stackexchange.com',\n",
       " u'https://bicycles.stackexchange.com',\n",
       " u'https://rpg.stackexchange.com',\n",
       " u'https://anime.stackexchange.com',\n",
       " u'https://puzzling.stackexchange.com',\n",
       " u'https://mechanics.stackexchange.com',\n",
       " u'https://stackexchange.com/sites#culturerecreation',\n",
       " u'https://mathoverflow.net',\n",
       " u'https://math.stackexchange.com',\n",
       " u'https://stats.stackexchange.com',\n",
       " u'https://cstheory.stackexchange.com',\n",
       " u'https://physics.stackexchange.com',\n",
       " u'https://chemistry.stackexchange.com',\n",
       " u'https://biology.stackexchange.com',\n",
       " u'https://cs.stackexchange.com',\n",
       " u'https://philosophy.stackexchange.com',\n",
       " u'https://stackexchange.com/sites#science',\n",
       " u'https://meta.stackexchange.com',\n",
       " u'https://stackapps.com',\n",
       " u'https://api.stackexchange.com',\n",
       " u'https://data.stackexchange.com',\n",
       " u'https://area51.stackexchange.com',\n",
       " u'https://stackoverflow.blog?blb=1',\n",
       " u'https://www.facebook.com/officialstackoverflow/',\n",
       " u'https://twitter.com/stackoverflow',\n",
       " u'https://linkedin.com/company/stack-overflow']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//ul/li/a/@href').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch individual item links from listing pages. I am going to modify code in parse_item method\n",
    "+ fetching links by using .css method of response. (other method than xpath)\n",
    "Crawling multiple pages \n",
    "following tutorial \n",
    "    + https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3\n",
    "    + https://medium.com/python-pandemonium/develop-your-first-web-crawler-in-python-scrapy-6b2ee4baf954\n",
    "top and bottom of each page has a little right carat (>) that links to the next page of results.\n",
    "tell the scraper to follow that link if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load medium/medium/spiders/news.py\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from medium.items import MediumItem\n",
    "\n",
    "class NewsSpider(CrawlSpider):\n",
    "    name = \"news\"\n",
    "    allowed_domains = [\"www.olx.com.pk\"]\n",
    "    start_urls = [\n",
    "        'https://www.olx.com.pk/computers-accessories/','https://www.olx.com.pk/games-entertainment/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=(), restrict_css=('.pageNextPrev',)),\n",
    "             callback=\"parse_item\",\n",
    "             follow=True),)\n",
    "    \n",
    "    def parse_item(self, response):\n",
    "        item_links = response.css('.large > .detailsLink::attr(href)').extract()\n",
    "        for a in item_links:\n",
    "            yield scrapy.Request(a, callback=self.parse_detail_page)\n",
    "\n",
    "\n",
    "\n",
    "    def parse_detail_page(self, response):\n",
    "        title = response.css('h1::text').extract()[0].strip()\n",
    "        item = MediumItem()\n",
    "        item['title'] = title\n",
    "        item['url'] = response.url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edit item.py by passing what you want to collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load medium/medium/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "\n",
    "class MediumItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    url = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output csv file with this command\n",
    "+ scrapy crawl news -o test4.csv -t csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load -r 1-20 medium/test4.csv \n",
    "\n",
    "https://www.olx.com.pk/item/zong-4g-IDWCPTz.html,Zong 4G\n",
    "https://www.olx.com.pk/item/google-advertising-ad-words-video-marketing-facebook-IDWCPTJ.html,Google Advertising - Ad Words / Video Marketing - Facebook\n",
    "https://www.olx.com.pk/item/hp-pavilion-15-core-i5-4th-gensilver-excellent-for-gaming-IDWCPSn.html,HP Pavilion 15 Core i5 4th Gen(Silver) Excellent For Gaming\n",
    "https://www.olx.com.pk/item/dell-inspiron-15-5570-core-i5-8th-gen-4gb-1tb-amd-new-2year-warranty-IDWCPS7.html,Dell Inspiron 15 5570 Core i5 8th Gen 4GB 1TB AMD NEW 2YEAR WARRANTY\n",
    "https://www.olx.com.pk/item/sapphire-hd-7850-dual-x-oc-edition-IDWCPTD.html,Sapphire hd 7850 dual-x oc edition\n",
    "https://www.olx.com.pk/item/brocade-foundry-netiron-ni-ces-2024c-l3prem-ac-IDUAgLU.html,Brocade Foundry NetIron NI-CES-2024C-L3PREM-AC\n",
    "https://www.olx.com.pk/item/facebook-advertising-digital-marketing-online-advertising-IDWCPU8.html,Facebook Advertising - Digital Marketing - Online Advertising\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrapy shell\n",
    "\n",
    "In a command window/ terminal window\n",
    "\n",
    "go to the tutorial project directory and type \n",
    "scrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n",
    "\n",
    "Try running the crawler to see the response using\n",
    "scrapy crawl dmoz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example crawler that creates a csv from:\n",
    "\n",
    "https://github.com/mjhea0/Scrapy-Samples/tree/master/crawlspider\n",
    "    \n",
    "I cloned the code as part of the code directory code/Scrapy-Samples-master\n",
    "and added a parent URL in the output csv file it can be run by using:\n",
    "\n",
    "cd to the code/Scrapy-Samples-master/crawlspider \n",
    "and run:\n",
    "scrapy crawl craigs -o items.csv -t csv\n",
    "\n",
    "Sample 20 lines from the csv file.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -r 1-20  medium/test4.csv\n",
    "url,title\n",
    "https://www.olx.com.pk/item/r9-280x-3gb-vapour-x-gaming-card-with-box-10-10-condition-IDWCPRv.html,R9 280x 3gb Vapour X Gaming Card With Box 10/10 Condition\n",
    "https://www.olx.com.pk/item/bluetooth-keyboard-blue-x5-IDWCPYw.html,Bluetooth Keyboard Blue X5\n",
    "https://www.olx.com.pk/item/dell-laptop-core-i5-3rd-generation-IDWCPNS.html,Dell Laptop core i5 3rd Generation\n",
    "https://www.olx.com.pk/item/lexmark-26-29-37-color-37xl-color-ink-cartridge-IDTnPpN.html,Lexmark 26/29/37 color 37xl color ink cartridge\n",
    "https://www.olx.com.pk/item/digital-marketing-social-media-manager-facebook-advertising-email-IDWCPUl.html,\"Digital Marketing - Social Media manager, Facebook Advertising, email\"\n",
    "https://www.olx.com.pk/item/hp-led-monitor-22-inches-IDWzTBP.html,Hp led monitor 22 inches\n",
    "https://www.olx.com.pk/item/bosch-pa-systems-llb4401-00-network-controller-pa-and-emergency-contro-IDUQSvR.html,Bosch PA Systems LLB4401/00 Network Controller PA and emergency contro\n",
    "https://www.olx.com.pk/item/stylish-computer-trolly-IDWzTBu.html,Stylish Computer trolly\n",
    "https://www.olx.com.pk/item/cisco-ws-c2950g-48-ei-ws-c2950g-24-ei-ws-c2960g-12-ei-IDUfebj.html,Cisco WS-C2950G-48-EI WS-C2950G-24-EI WS-C2960G-12-EI\n",
    "https://www.olx.com.pk/item/hp-pavilion-dv6-3300se-final-price-no-negotiations-IDW2pn3.html,\"HP Pavilion dv6 3300se - Final Price, No Negotiations\"\n",
    "https://www.olx.com.pk/item/num-pad-ibm-delivery-available-IDWCPXi.html,Num Pad IBM Delivery Available\n",
    "https://www.olx.com.pk/item/lenovo-laptop-IDWCPV9.html,Lenovo laptop\n",
    "https://www.olx.com.pk/item/jctech-multimedia-subwoofer-system-IDWCPO9.html,Jctech multimedia subwoofer system\n",
    "https://www.olx.com.pk/item/hp-spectre-13w023-ci7-7gen-16gb-512ssd-13-3-x360-convertible-touch-IDWCPYi.html,\"HP Spectre 13w023 Ci7 7gen 16gb 512ssd 13.3\"\" x360 Convertible Touch\"\n",
    "https://www.olx.com.pk/item/asus-amd-a10-5750m-5th-series-IDWCPOa.html,Asus amd A10 5750m 5th series\n",
    "https://www.olx.com.pk/item/dell-vostro-14-i7-7th-gen-IDWCPOh.html,Dell Vostro 14 i7 7th Gen\n",
    "https://www.olx.com.pk/item/ps4-batman-arkham-knight-region-2-exchange-possible-IDWuGvo.html,[PS4] Batman Arkham Knight region 2 exchange possible\n",
    "https://www.olx.com.pk/item/overwatch-region-2-ps4-game-IDWAvx6.html,Overwatch Region 2 ps4 Game\n",
    "https://www.olx.com.pk/item/sony-psp-play-station-IDWCnFk.html,Sony PSP play station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.olx.com.pk/item/ps4-pro-1tb-region...</td>\n",
       "      <td>Ps4 pro 1tb Region 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.olx.com.pk/item/evil-within-2-IDWC...</td>\n",
       "      <td>Evil within 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.olx.com.pk/item/overwatch-region-2...</td>\n",
       "      <td>Overwatch Region 2 ps4 Game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.olx.com.pk/item/xbox-360-slim-500g...</td>\n",
       "      <td>Xbox 360 slim 500gb with 50 games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.olx.com.pk/item/tritton-kama-heads...</td>\n",
       "      <td>Tritton Kama headset for ps4.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.olx.com.pk/item/ps4-pro-1tb-region...   \n",
       "1  https://www.olx.com.pk/item/evil-within-2-IDWC...   \n",
       "2  https://www.olx.com.pk/item/overwatch-region-2...   \n",
       "3  https://www.olx.com.pk/item/xbox-360-slim-500g...   \n",
       "4  https://www.olx.com.pk/item/tritton-kama-heads...   \n",
       "\n",
       "                               title  \n",
       "0               Ps4 pro 1tb Region 2  \n",
       "1                      Evil within 2  \n",
       "2        Overwatch Region 2 ps4 Game  \n",
       "3  Xbox 360 slim 500gb with 50 games  \n",
       "4      Tritton Kama headset for ps4.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install graphistry using \n",
    "+ pip install graphistry \n",
    "+ I got the key from graphisty to visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# links', 154)\n",
      "('# event entities', 77)\n",
      "('# attrib entities', 154)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <iframe id=\"853a1f3f-96c9-4cf2-848a-8b724d73c411\" src=\"https://labs.graphistry.com/graph/graph.html?dataset=PyGraphistry/0E47IUL5KG&type=vgraph&viztoken=a8a7fe905d06f1646de30fdeacb42128dd35683c&usertag=9d1fe893-pygraphistry-0.9.51&splashAfter=1511723639&info=true\"\n",
       "                    allowfullscreen=\"true\" webkitallowfullscreen=\"true\" mozallowfullscreen=\"true\"\n",
       "                    oallowfullscreen=\"true\" msallowfullscreen=\"true\"\n",
       "                    style=\"width:100%; height:500px; border: 1px solid #DDD\">\n",
       "            </iframe>\n",
       "        \n",
       "            <script>\n",
       "                $(\"#853a1f3f-96c9-4cf2-848a-8b724d73c411\").bind('mousewheel', function(e) {\n",
       "                e.preventDefault();\n",
       "                });\n",
       "            </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import graphistry\n",
    "graphistry.register(key='bc998b24924334858e23c47d7c05e6f286d7e4e267f95461053655516a53736e')\n",
    "rows = pandas.read_csv('test4.csv')[1:100]\n",
    "\n",
    "graphistry.hypergraph(rows)['graph'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank\n",
    "\n",
    "Using Dileep Kumars implementation: available at http://buildsearchengine.blogspot.com/2012/04/page-rank-algorithm.html\n",
    "\n",
    "+ The compute_ranks function computes the ranks of the pages.\n",
    "+ A graph, which contains pages as keys and all the links that occur in each page as their corresponding url lists. \n",
    "+ A Look_up_new function is defined that first shows the ranking of the returned url's that contain the keyword\n",
    "+ urls are sorted using the QuickSort algorithm and arranged in descending order \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#Author : dileep98490@gmail.com\n",
    "\n",
    "import urllib\n",
    "max_limit=200\n",
    "\n",
    "def get_page(url):#This function is just to return the webpage contents; the source of the webpage when a url is given.\n",
    "\n",
    "    try:\n",
    "        f = urllib.urlopen(url)\n",
    "        page = f.read()\n",
    "        f.close()\n",
    "        #print page\n",
    "        return page\n",
    "    except:\t\n",
    "        return \"\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The union function merges the second list into first, with out duplicating an element of a, if it's already in a.\n",
    "#Similar to set union operator. This function does not change b. \n",
    "#If a=[1,2,3] b=[2,3,4]. After union(a,b) makes a=[1,2,3,4] and b=[2,3,4]\n",
    "def union(a,b):\n",
    "    for e in b:\n",
    "        if e not in a:\n",
    "            a.append(e)\n",
    "\n",
    "def get_next_url(page):\n",
    "    start_link=page.find(\"a href\")\n",
    "    if(start_link==-1):\n",
    "        return None,0\n",
    "\n",
    "    start_quote=page.find('\"',start_link)\n",
    "    end_quote=page.find('\"',start_quote+1)\n",
    "    url=page[start_quote+1:end_quote]\n",
    "    return url,end_quote\n",
    "\n",
    "def get_all_links(page):\n",
    "    links=[]\n",
    "    while(True):\n",
    "        url,n=get_next_url(page)\n",
    "        page=page[n:]\n",
    "        if url:\n",
    "            links.append(url)\n",
    "        else:\n",
    "            break\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is for given an index, it finds the keyword in the index and returns the list of links\n",
    "def Look_up(index,keyword):\n",
    "    #f=[]\n",
    "    if keyword in index:\n",
    "        return index[keyword]\n",
    "    return []\n",
    "\n",
    "#The format of element in the index is <keyword>,[<List of urls that contain the keyword>]\n",
    "def add_to_index(index,url,keyword):\n",
    "    if keyword in index:\n",
    "        if url not in index[keyword]:\n",
    "            index[keyword].append(url)\n",
    "        return\n",
    "    index[keyword]=[url]\n",
    "\n",
    "#Adding the content of the webpage to the index\n",
    "def add_page_to_index(index,url,content):\n",
    "    for i in content.split():\n",
    "        add_to_index(index,url,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing ranks for a given graph -> for all the links in it\n",
    "def compute_ranks(graph):\n",
    "    d=0.8\n",
    "    numloops=10\n",
    "    ranks={}\n",
    "    npages=len(graph)\n",
    "\n",
    "    for page in graph:\n",
    "        ranks[page]=1.0/npages\n",
    "\n",
    "    for i in range(0,numloops):\n",
    "        newranks={}\n",
    "        for page in graph:\n",
    "            newrank=(1-d)/npages\n",
    "            for node in graph:\n",
    "                if page in graph[node]:\n",
    "                    newrank=newrank+d*ranks[node]/len(graph[node])\n",
    "            newranks[page]=newrank\n",
    "        ranks=newranks\n",
    "    return ranks\n",
    "\n",
    "def Crawl_web(seed):#The website to act as seed page is given as input\n",
    "\n",
    "    tocrawl=[seed]\n",
    "    crawled=[]\n",
    "    index={}\n",
    "    graph={}#new graph\n",
    "    global max_limit\n",
    "\n",
    "    while tocrawl:\n",
    "        p=tocrawl.pop()\n",
    "        #To remove the looping, if a page is already crawled and it is backlinked again by someother link we are crawling, we need not crawl it again\n",
    "        if p not in crawled:\n",
    "            max_limit-=1\n",
    "            print max_limit\n",
    "            if max_limit<=0:\n",
    "                break\n",
    "            c=get_page(p)\n",
    "            add_page_to_index(index,p,c)\n",
    "            f=get_all_links(c)\n",
    "            union(tocrawl,f)\n",
    "            graph[p]=f\n",
    "            crawled.append(p)#As soon as a link is crawled it is appended to crawled. In the end when all the links are over, we will return the crawled since it contains all the links we have so far\n",
    "\n",
    "    return crawled,index,graph #Returns the list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def QuickSort(pages,ranks):#Sorting in descending order\n",
    "\n",
    "    if len(pages)>1:\n",
    "        piv=ranks[pages[0]]\n",
    "        i=1\n",
    "        j=1\n",
    "        for j in range(1,len(pages)):\n",
    "            if ranks[pages[j]]>piv:\n",
    "                pages[i],pages[j]=pages[j],pages[i]\n",
    "                i+=1\n",
    "        pages[i-1],pages[0]=pages[0],pages[i-1]\n",
    "        QuickSort(pages[1:i],ranks)\n",
    "        QuickSort(pages[i+1:len(pages)],ranks)\n",
    "\n",
    "\n",
    "\n",
    "def Look_up_new(index,ranks,keyword):\n",
    "\n",
    "    pages=Look_up(index,keyword)\n",
    "    print '\\nPrinting the results as is with page rank\\n'\n",
    "    for i in pages:\n",
    "        print i+\" --> \"+str(ranks[i])#Displaying the lists, so that you can see the page rank along side\n",
    "\n",
    "    QuickSort(pages,ranks)\n",
    "    print \"\\nAfter Sorting the results by page rank\\n\"\n",
    "    it=0\n",
    "    for i in pages:#This is how actually it looks like in search engine results - > sorted by page rank\n",
    "        it+=1\n",
    "        print str(it)+'.\\t'+i+'\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the seed page\n",
      "https://www.olx.com.pk/computers-accessories/\n",
      "Enter What you want to search\n",
      "is\n",
      "Enter the depth you wanna go\n",
      "12\n",
      "\n",
      "Started crawling, presently at depth..\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "\n",
      "Printing the results as is with page rank\n",
      "\n",
      "https://www.olx.com.pk/computers-accessories/ --> 0.0181818181818\n",
      "https://www.olx.com.pk/myaccount/ --> 0.0214974217391\n",
      "https://www.olx.com.pk/account/register/ --> 0.0196792399209\n",
      "https://itunes.apple.com/app/olx-pakistan/id1119081665?mt=8 --> 0.0196792399209\n",
      "http://eclipse.org/jetty --> 0.0329605081645\n",
      "https://www.eclipse.org --> 0.0189000855001\n",
      "https://www.linkedin.com/company/eclipse-foundation --> 0.0185402406478\n",
      "https://status.eclipse.org --> 0.0183732114527\n",
      "https://wiki.eclipse.org/IT_Infrastructure_Doc#Use_mirror_sites.2Fsee_which_mirrors_are_mirroring_my_files.3F --> 0.0183488473768\n",
      "\n",
      "After Sorting the results by page rank\n",
      "\n",
      "1.\thttps://wiki.eclipse.org/IT_Infrastructure_Doc#Use_mirror_sites.2Fsee_which_mirrors_are_mirroring_my_files.3F\n",
      "\n",
      "2.\thttps://www.olx.com.pk/myaccount/\n",
      "\n",
      "3.\thttps://www.olx.com.pk/account/register/\n",
      "\n",
      "4.\thttps://itunes.apple.com/app/olx-pakistan/id1119081665?mt=8\n",
      "\n",
      "5.\thttp://eclipse.org/jetty\n",
      "\n",
      "6.\thttps://www.eclipse.org\n",
      "\n",
      "7.\thttps://www.linkedin.com/company/eclipse-foundation\n",
      "\n",
      "8.\thttps://status.eclipse.org\n",
      "\n",
      "9.\thttps://www.olx.com.pk/computers-accessories/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print index\n",
    "print \"Enter the seed page\"\n",
    "seed_page=raw_input()\n",
    "\n",
    "print \"Enter What you want to search\"\n",
    "search_term=raw_input()\n",
    "\n",
    "try:\n",
    "    print \"Enter the depth you wanna go\"\n",
    "    max_limit=int(raw_input())\n",
    "except:\n",
    "    f=None\n",
    "print '\\nStarted crawling, presently at depth..'\n",
    "\n",
    "crawled,index,graph=Crawl_web(seed_page)#printing all the links\n",
    "ranks=compute_ranks(graph)#Calculating the page ranks\n",
    "Look_up_new(index,ranks,search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
